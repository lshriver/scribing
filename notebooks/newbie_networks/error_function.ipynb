{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Error Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared error (MSE)\n",
    "The error is the difference between the actual value and the predicted value.\n",
    "- Minimizing this value gives us the best possible model.\n",
    "- Error equation:\n",
    "    $$\\text{error}=y-y'$$\n",
    "📝 In order to avoid negative errors (which don't make sense), we square the error function:\n",
    "$$E = (y-y')^2$$\n",
    "\n",
    "Here, \n",
    "- $y=$ the actual output\n",
    "- $y'=$ the predicted output\n",
    "\n",
    "We get the **mean squared error** by halving $E$:\n",
    "$$MSE=\\frac{1}{2}(y-y')^2$$\n",
    "\n",
    "✨ The mean squared error is a common choice as a distance metric for regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy (CE)\n",
    "✨ Common choice as a distance metric for classification. \n",
    "\n",
    "For a binary classification task, the **cross-entropy** is defined as\n",
    "$$CE = -\\bigg(y\\log(y')+(1-y)\\log(1-y')\\bigg)$$\n",
    "where,\n",
    "- $y=$ the actual output\n",
    "- $y'=$ the predicted output\n",
    "  \n",
    "📝 The cross-entropy is also called **log loss**.\n",
    "- Minimizing the cross-entropy corresponds with maximizing the log-likelihood of a predicted value belonging to a particular class.\n",
    "- A lower loss indicates better performance, meaning the predicted probability is closer to the actual label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'typeing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtypeing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Union\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(x: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"The sigmoid activation function\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'typeing'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typeing import Union\n",
    "\n",
    "def sigmoid(x: Union[int, float]) -> float:\n",
    "    \"\"\"\"The sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-x)) # applying the sigmoid activation function\n",
    "\n",
    "def forward_propagation(input_data: np.ndarray, weights: np.ndarray, bias: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the forward propagation operation of a perceptron and \n",
    "    returns the output after applying the sigmoid activation function\n",
    "    \"\"\"\n",
    "    # take the dot product of input and weight and add the bias\n",
    "    return sigmoid(np.dot(input_data, weights) + bias) # the perceptron equation\n",
    "\n",
    "def calculate_error(y: np.ndarray, y_predicted: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\"Computes the binary cross entropy error\"\"\"\n",
    "    # the cross entropy function\n",
    "    return - y * np.log(y_predicted) - (1 - y) * np.log(1 - y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value:  0.289050497374996\n",
      "Error:  [0.34115387]\n"
     ]
    }
   ],
   "source": [
    "# initialize values\n",
    "X = np.array([2, 3]) # declaring two data points\n",
    "Y = np.array([0]) # label\n",
    "weights = np.array([1.0, -1.0]) # weights of perceptron\n",
    "bias = 0.1 # bias value\n",
    "Y_predicted = forward_propagation(X, weights.T, bias) # predicted label\n",
    "print(\"Predicted value: \", Y_predicted)\n",
    "error = calculate_error(Y, Y_predicted)\n",
    "print(\"Error: \", error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
